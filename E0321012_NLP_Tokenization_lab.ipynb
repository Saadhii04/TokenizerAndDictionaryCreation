{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc5MHW2yWR7V",
        "outputId": "c5ca66c1-9839-4221-b5d5-293b38cf4d3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jmE3CyoHU_-V"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"This is a sample paragraph. It contains several sentences. Each sentence will be tokenized into words.\""
      ],
      "metadata": {
        "id": "eiB2MnPyWYWQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "4zowiIoFXktd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n"
      ],
      "metadata": {
        "id": "S3dtDsPmXlQ1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in tokenized_sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNa6RvbzXocW",
        "outputId": "caabd0da-fb16-40b9-9abd-ebd50c656fa0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'paragraph', '.']\n",
            "['It', 'contains', 'several', 'sentences', '.']\n",
            "['Each', 'sentence', 'will', 'be', 'tokenized', 'into', 'words', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set([word for sentence in tokenized_sentences for word in sentence])"
      ],
      "metadata": {
        "id": "43ypSgyhXrK1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nVocabulary:\", vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AlvwItFXxpV",
        "outputId": "a75bc7cb-dc4f-48e4-d64f-961a1b560a1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary: {'This', 'be', 'sentence', 'words', 'tokenized', 'will', 'sample', 'sentences', 'Each', 'is', 'paragraph', '.', 'into', 'several', 'contains', 'It', 'a'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [\n",
        "    \"I don't like bananas\",\n",
        "    \"They're always late\",\n",
        "    \"This was co-created by my friend\",\n",
        "    \"This document was signed \\\"12-02-24\\\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "XoqC04EcYESk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
        "\n",
        "vocabulary = set([word for sentence in tokenized_sentences for word in sentence])"
      ],
      "metadata": {
        "id": "92DgsNTJZwkV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenized Sentences:\")\n",
        "for sentence in tokenized_sentences:\n",
        "    print(sentence)\n",
        "\n",
        "print(\"\\nVocabulary:\", vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3Pa_mL5Zzfh",
        "outputId": "35139652-1eb9-4352-9251-90fde1d225c1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentences:\n",
            "['I', \"don't\", 'like', 'bananas']\n",
            "[\"They're\", 'always', 'late']\n",
            "['This', 'was', 'co-created', 'by', 'my', 'friend']\n",
            "['This', 'document', 'was', 'signed', '\"12-02-24\"']\n",
            "\n",
            "Vocabulary: {'This', 'signed', 'friend', 'late', \"don't\", 'by', 'I', 'document', 'always', \"They're\", 'was', 'co-created', 'like', 'my', '\"12-02-24\"', 'bananas'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_7tlzFSZ7Kj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}